---
---
@string{ieee = {IEEE}}

@inproceedings{xiang2023towards,
selected=true,
abbr={NeurIPS 2024},
bibtex_show={true},
booktitle={The 38th Annual Conference on Neural Information Processing Systems (NeurIPS)},
title={Efficient Federated Learning against Heterogeneous and Non-stationary Client Unavailability},
author={Xiang, Ming and Ioannidis, Stratis and Yeh, Edmund and Joe-Wong, Carlee and Su, Lili},
year={2024},
abstract={
<p>
Addressing intermittent client availability is critical for the real-world deployment of federated learning algorithms. 
Most prior work either overlooks the potential non-stationarity in the dynamics of client unavailability or requires substantial memory/computation overhead. 
We study federated learning in the presence of heterogeneous and non-stationary client availability, which may occur when the deployment environments are uncertain or the clients are mobile. 
The impacts of the heterogeneity and non-stationarity in client unavailability can be significant, as we illustrate using FedAvg, the most widely adopted federated learning algorithm. 
We propose FedAPM, which includes novel algorithmic structures that (i) compensate for missed computations due to unavailability with only <it>O(1)</it> additional memory and computation with respect to standard FedAvg, and (ii) evenly diffuse local updates within the federated learning system through implicit gossiping, despite being agnostic to non-stationary dynamics. 
We show that FedAPM converges to a stationary point of even non-convex objectives while achieving the desired linear speedup property. 
We corroborate our analysis with numerical experiments over diversified client unavailability dynamics on real-world data sets.
</p>
},
arxiv={2409.17446}
}


@article{xiang2024empowering,
abbr={Submitted},
title={Empowering Federated Learning with Implicit Gossiping: Mitigating Connection Unreliability Amidst Unknown and Arbitrary Dynamics},
author={Xiang, Ming and Ioannidis, Stratis and Yeh, Edmund and Joe-Wong, Carlee and Su, Lili},
year={2024},
publisher={IEEE},
arxiv={2404.10091}
}

@article{Su2023adversarial,
abbr={Submitted},
title={Federated Learning in the Presence of Adversarial Client Unavailability},
author={Su, Lili and Xiang, Ming and Xu, Jiaming and Yang, Pengkun},
year={2023},
publisher={IEEE},
arxiv={2305.19971}
}

@inproceedings{xiang2023towards,
selected=true,
abbr={CDC 2023},
bibtex_show={true},
booktitle={2023 IEEE 62st Conference on Decision and Control (CDC)},
title={Towards Bias Correction of FedAvg over Nonuniform and Time-Varying Communications},
author={Xiang, Ming and Ioannidis, Stratis and Yeh, Edmund and Joe-Wong, Carlee and Su, Lili},
year={2023},
abstract={
<p>
Federated learning (FL) is a decentralized machine learning framework wherein a parameter server (PS) and a collection of clients collaboratively 
trains a model.  
Communication bandwidth is a scarce resource. In each round, the PS aggregates the updates from a subset of clients only. 
In this paper, we consider non-uniform and  time-varying communication between the PS and the clients. Specifically, in each round <i>t</i>, the link between the PS and client <i>i</i> is active with probability <it>p</it><sub>i</sub><sup>t</sup>, which is unknown to both the PS and the clients. 
This arises when 
the channel conditions are heterogeneous across clients and are changing over time.  

We show that when the <i>p</i><sub>i</sub><sup>t</sup>'s are not uniform (i.e., not identical over <i>i</i>), Federated Average (FedAvg) -- the most widely adopted FL algorithm -- fails to minimize the global objective. Observing this, we propose Federated Postponed Broadcast (FedPBC) which is a simple variant of FedAvg; it differs from FedAvg in that the PS postpones broadcasting the global model till the end of each round.  
FedPBC converges to a stationary point. 
Moreover, the staleness is mild and there is no significant slowdown. Both theoretical analysis and numerical results are provided. 
On the technical front,
postponing the global model broadcasts enables implicit gossiping among the clients with active links at round <i>t</i>. 
Consequently, we are able to control the perturbation of the global model dynamics caused by non-uniform and time-varying <i>p</i><sub>i</sub><sup>t</sup> via the techniques of controlling gossip-type information mixing errors.
</p>
},
arxiv={2306.00280},
publisher = ieee
}

@article{xiang2020state,
abbr={IEEE Access},
bibtex_show={true},
title={State-of-health prognosis for lithium-ion batteries considering the limitations in measurements via maximal information entropy and collective sparse variational gaussian process},
author={Xiang, Ming and He, Yigang and Zhang, Hui and Zhang, Chaolong and Wang, Lei and Wang, Chenyuan and Sui, Chunsong},
journal={IEEE Access},
volume={8},
pages={188199--188217},
year={2020},
publisher={IEEE},
html={https://ieeexplore.ieee.org/document/9216154}
}

@article{xiang2023federateddp,
abbr={Submitted},
title={Federated SGD with Differentially Private and Byzantine Resilient One-Bit Compressors},
author={Xiang, Ming and Su, Lili},
year={2023},
publisher={IEEE},
arxiv={2210.00665}
}